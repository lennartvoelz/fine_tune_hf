{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lennartvoelz/fine_tune_hf/blob/main/FunctionGemma_(270M)_finetune_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdS3fN5lUcL"
      },
      "source": [
        "### Install Unsloth and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBeBCkyhmONg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.3\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbJTyUJ5wluY"
      },
      "outputs": [],
      "source": [
        "!pip install modelscope\n",
        "import json\n",
        "os.environ['UNSLOTH_USE_MODELSCOPE'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xbb0cuLzwgf"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "max_seq_length = 256\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"google/functiongemma-270m-it\",\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = True, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    load_in_16bit = False, # [NEW!] Enables 16bit LoRA\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    token = hf_token, # HF Token for gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 42,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWxBiHgSlUcM"
      },
      "source": [
        "Formatting is very important in the `functiongemma` for tool-calling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwG8WobYOI2d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/data/\"\n",
        "\n",
        "raw_data = []\n",
        "with open(SAVE_DIR+\"examples.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            raw_data.append(json.loads(line))\n",
        "\n",
        "def format_cactus_call(expected_calls):\n",
        "    \"\"\"Replicates the Cactus C++ expected output format\"\"\"\n",
        "    if not expected_calls:\n",
        "        return \"\"\n",
        "\n",
        "    call = expected_calls[0]\n",
        "    result = f\"<start_function_call>call:{call['name']}{{\"\n",
        "\n",
        "    args = []\n",
        "    for k, v in call[\"arguments\"].items():\n",
        "        if isinstance(v, str):\n",
        "            args.append(f\"{k}:<escape>{v}<escape>\")\n",
        "        else:\n",
        "            args.append(f\"{k}:{v}\")\n",
        "\n",
        "    result += \",\".join(args) + \"}<end_function_call>\"\n",
        "    return result\n",
        "\n",
        "def format_cactus_tools(tools_list):\n",
        "    \"\"\"Replicates the Cactus format_tools() C++ function\"\"\"\n",
        "    result = \"\"\n",
        "    for t in tools_list:\n",
        "        result += \"<start_function_declaration>\"\n",
        "        result += f\"declaration:{t['name']}{{\"\n",
        "\n",
        "        desc = t.get('description', '')\n",
        "        result += f\"description:<escape>{desc}<escape>\"\n",
        "\n",
        "        if \"parameters\" in t and \"properties\" in t[\"parameters\"]:\n",
        "            result += \",parameters:{properties:{\"\n",
        "            props = []\n",
        "            for k, v in t[\"parameters\"][\"properties\"].items():\n",
        "                p_desc = v.get('description', '')\n",
        "                p_type = v.get('type', 'string').upper()\n",
        "                prop_str = f\"{k}:{{description:<escape>{p_desc}<escape>,type:<escape>{p_type}<escape>}}\"\n",
        "                props.append(prop_str)\n",
        "            result += \",\".join(props) + \"}\"\n",
        "\n",
        "            if \"required\" in t[\"parameters\"]:\n",
        "                reqs = [f\"<escape>{r}<escape>\" for r in t[\"parameters\"][\"required\"]]\n",
        "                result += f\",required:[{','.join(reqs)}]\"\n",
        "\n",
        "            t_type = t['parameters'].get('type', 'object').upper()\n",
        "            result += f\",type:<escape>{t_type}<escape>}}\"\n",
        "\n",
        "        result += \"}<end_function_declaration>\"\n",
        "    return result\n",
        "\n",
        "def format_tools_for_prompt(example):\n",
        "    user_msg = example[\"messages\"][0][\"content\"]\n",
        "    expected_calls = example.get(\"expected_calls\", [])\n",
        "    tools_list = example.get(\"tools\", [])\n",
        "\n",
        "    # --- 1. BUILD THE SYSTEM PROMPT (EXACTLY AS C++ DOES) ---\n",
        "    system_content = \"You are a helpful assistant that can use tools.\\n\"\n",
        "\n",
        "    if tools_list:\n",
        "        tools_json = format_cactus_tools(tools_list)\n",
        "        system_content += \"You are a model that can do function calling with the following functions.\"\n",
        "        system_content += tools_json\n",
        "        system_content += \"\\n\\nWhen you decide to call a function, output it in this exact format:\\n\"\n",
        "        system_content += \"<start_function_call>call:function_name{arg1:<escape>value1<escape>,arg2:<escape>value2<escape>}<end_function_call>\"\n",
        "\n",
        "    # --- 2. BUILD THE ASSISTANT CONTENT (EXACTLY AS C++ EXPECTS) ---\n",
        "    if expected_calls:\n",
        "        assistant_content = format_cactus_call(expected_calls)\n",
        "    else:\n",
        "        assistant_content = example.get(\"expected_response\", \"\")\n",
        "\n",
        "    # --- 3. CONSTRUCT THE FULL CHAT STRING MANUALLY (BYPASSING JINJA/HF) ---\n",
        "    # We build the exact string Gemma needs without apply_chat_template\n",
        "\n",
        "    chat_str = (\n",
        "        f\"<start_of_turn>developer\\n\"\n",
        "        f\"{system_content}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>user\\n\"\n",
        "        f\"{user_msg}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n\"\n",
        "        f\"{assistant_content}\"\n",
        "    )\n",
        "\n",
        "    # Notice we DO NOT add a final <end_of_turn>\\n here because we want the model\n",
        "    # to learn to generate it after outputting the function call!\n",
        "\n",
        "    return {\n",
        "        \"text\": chat_str\n",
        "    }\n",
        "\n",
        "chat_data = []\n",
        "\n",
        "for example in raw_data:\n",
        "    formatted_example = format_tools_for_prompt(example)\n",
        "    chat_data.append(formatted_example)\n",
        "\n",
        "with open(\"chat_ready.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for ex in chat_data:\n",
        "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRXvIhAFLhBr"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"chat_ready.jsonl\", split=\"train\")\n",
        "\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 500 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2, # Use GA to mimic batch size!\n",
        "        warmup_steps = 2,\n",
        "        #num_train_epochs = 2, # Set this for 1 full training run.\n",
        "        max_steps = 15,\n",
        "        learning_rate = 8e-5, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.007,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l35FwgQ1phi6"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA33AvDwsmn-"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtWXmRJplcA"
      },
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDvlCjbS7l86"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[-1][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ltHtqSpoHg"
      },
      "source": [
        "Now let's print the masked out example - you should see only the answer is present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjwHcW-X7mj4"
      },
      "outputs": [],
      "source": [
        "[tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[25][\"labels\"]]).replace(tokenizer.pad_token, \"-\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNP1Uidk9mrz"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Grab the raw example from your data\n",
        "example = raw_data[0]\n",
        "user_msg = example[\"messages\"][0][\"content\"]\n",
        "tools_list = example.get(\"tools\", [])\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "end_call_token_id = tokenizer.convert_tokens_to_ids(\"<end_function_call>\")\n",
        "\n",
        "# 2. Build the exact Cactus System Prompt (reusing our logic from before)\n",
        "system_content = \"You are a helpful assistant that can use tools.\\n\"\n",
        "if tools_list:\n",
        "    tools_json = format_cactus_tools(tools_list) # Make sure this function is defined!\n",
        "    system_content += \"You are a model that can do function calling with the following functions.\"\n",
        "    system_content += tools_json\n",
        "    system_content += \"\\n\\nWhen you decide to call a function, output it in this exact format:\\n\"\n",
        "    system_content += \"<start_function_call>call:function_name{arg1:<escape>value1<escape>,arg2:<escape>value2<escape>}<end_function_call>\"\n",
        "\n",
        "# 3. Manually construct the prompt string for generation\n",
        "# Notice we add \"<start_of_turn>model\\n\" at the end, which acts as the \"add_generation_prompt=True\"\n",
        "text = (\n",
        "    f\"<bos><start_of_turn>system\\n\"\n",
        "    f\"{system_content}<end_of_turn>\\n\"\n",
        "    f\"<start_of_turn>user\\n\"\n",
        "    f\"{user_msg}<end_of_turn>\\n\"\n",
        "    f\"<start_of_turn>model\\n\"\n",
        ")\n",
        "\n",
        "# 4. Tokenize and prepare for Unsloth / HF Generation\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "# skip_prompt=True keeps the output clean so you only see the generated function call\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# 5. Generate with strict deterministic settings\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    streamer=streamer,\n",
        "    do_sample=False,   # Greedy decoding is required for JSON/Function calls\n",
        "    temperature=0.0,\n",
        "    eos_token_id=[eos_token_id, end_call_token_id]\n",
        ")"
      ],
      "metadata": {
        "id": "XsR0O7l5HtcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "id": "8_G-14_Z66r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(SAVE_DIR+\"functiongemma_lora\")  # Local saving\n",
        "tokenizer.save_pretrained(SAVE_DIR+\"functiongemma_lora\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}